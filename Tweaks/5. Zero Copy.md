# Zero Copy

Zero copy是一个老生常谈的概念了, 其本质是节约CPU. 当然, 也可能避免了不必要的内存分配, 因为在复制的过程中, 通常需要分配新的内存来容纳需要复制的数据. 一般的, 假设CPU每个cycle能复制1个byte（最好情况）, 那么, 假设CPU频率是2.7GHz, 其吞吐为2.7GB/s. 看起来这是一个很大的值, 然而: 

+ 考虑到CPU还需要做别的事情,且2.7GB/s是极端最优情况,在现实生活中根本不可能出现
+ 25G网卡已经投入使用,100G网卡已经Ready,然而CPU在频率上基本没有任何进步

## 避免层到层之间的数据复制

上层调用下层, 下层回调上层, 通常都会伴随着数据交换, 而数据交换又会带来新内存分配与数据复制. 比如, 

+ 执行一个```socket write```时, 数据会先被复制到内核缓冲区, 再从内核缓冲区复制到网卡. 
+ 执行一个非direct文件读时, 在缓存未命中的情况下, 内核会先在```PageCache```中分配对应的```cache```（内存分配）, 然后发起外部设备读请求, 数据被填充到```PageCache```中,  会再被复制到用户提供的缓存中. ([Buffered IO和Direct IO](https://blog.csdn.net/batmannolove/article/details/50435597))
+ 使用```C stdio```时, 默认情况下, 每个```FILE```也会维护一个缓存. 此缓存的目标是减少系统调用的开销, 代价是一次额外的复制. 然而幸运的是, 它的缓存空间是静态的, 不会涉及到额外的内存分配.
+ 使用```C stdio```时, 认情况下, 每个FILE也会维护一个缓存. 此缓存的目标是减少系统调用的开销, 代价是一次额外的复制. 然而幸运的是, 它的缓存空间是静态的, 不会涉及到额外的内存分配.

要避免层到层之间的数据复制, 你需要对```I/O```链路上的每一步都十分清楚, 这样才能有的放矢

+ 如何避免socket的复制开销? 使用用户态TCP栈, 数据直接从网卡进应用内存, 直接从应用内存到网卡
+ 如何避免磁盘文件的复制开销? 使用direct读, 一般需要配合应用态缓存[__知乎文章:聊聊Linux IO](https://zhuanlan.zhihu.com/p/71149410)
+ 如何避免```C stdio```的复制开销? 在对自己的数据模式很了解, 以及, 了解```C stdio```缓存的存在意义的前提下, 关闭```C stdio```的缓存

## 避免数据结构扩张时带来的数据复制

一般的, 在进行数据交换的时候, 如果要处理的数据的单元是固定大小的, 我们可以使用一个固定大小的buffer来交换数据. 比如daytime协议, 我们```明确知道```服务器返回的应用数据包的大小, 因此可以事先分配好一个buffer. 

但是, 很多协议中, 数据包是变长的. 运气好的, 它会在包的开头表明整个包的大小, 因此, 你可以先读入一个header, 了解包的大小, 然后分配对应大小的buffer, 再将对应payload内容读入. 运气不好的, 你需要读到某个休止符, 比如HTTP中的\r\n. 此时, 显然你需要动态扩容你的buffer（当然, 也可以事先决定数据包的最大大小, 然后按照最大大小来分配buffer）. 然而, 在动态扩容的过程中, 我们除了要分配新的内存, 还需要将旧的内存上的数据重新复制到新的内存上. 这个过程类型C std中的realloc. 


解决此问题的方法是加一个抽象层: 我们不需要一个大的、连续的buffer, 我们可以将很多小的buffer组合起来, 然后对外提供一个大的、连续buffer的抽象. 例子可以参考linux tcp/ip实现中的skbuf. 

## 将复制工作交给其他硬件

前面提到, Zero copy的本质是节约CPU, 因此, 如果由其他硬件来完成复制, 不就ok了吗？当然, 复制本身的延迟是避免不了的. 典型的硬件复制技术DMA






### reference

+ [direct I/O](https://stuff.mit.edu/afs/athena/project/rhel-doc/5/RHEL-5-manual/Global_File_System/s1-manage-direct-io.html)
+ [用户进程缓冲区和内核缓冲区](http://www.pulpcode.cn/2017/02/01/user-buffer-and-kernel-buffer/)
+ [聊聊linux I/O](https://zhuanlan.zhihu.com/p/71149410)